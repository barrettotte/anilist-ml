{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data\n",
    "\n",
    "Fetches all Anilist data needed for practicing data visualization and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from anilist_fetch import fetch as anilist\n",
    "\n",
    "# setup\n",
    "data_dir = os.path.abspath('data')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "today = datetime.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'barrettotte', 'createdAt': 1552085204, 'statistics': {'anime': {'count': 605, 'meanScore': 68.93, 'standardDeviation': 15.9, 'minutesWatched': 198339, 'episodesWatched': 8119}}}\n"
     ]
    }
   ],
   "source": [
    "ANILIST_USER_ID = 247578\n",
    "user = anilist.get_user(ANILIST_USER_ID)\n",
    "print(user['user'])\n",
    "\n",
    "# write user anime entries to CSV\n",
    "with open(os.path.join(data_dir, f\"user-{ANILIST_USER_ID}-{today}.csv\"), 'w+', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['media_id', 'status', 'score', 'progress', 'completedAt'])\n",
    "\n",
    "    for media_list in user['lists']:\n",
    "        for entry in media_list['entries']:\n",
    "            data = [entry['media']['id'], entry['status'], entry['score'], entry['progress'], None]\n",
    "            completed_at = entry['completedAt']\n",
    "\n",
    "            if completed_at['year'] and completed_at['month'] and completed_at['day']:\n",
    "                data[4] = f\"{completed_at['month']:02d}-{completed_at['day']:02d}-{completed_at['year']}\"\n",
    "\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch All Anime Entries\n",
    "\n",
    "Takes around 10-15 minutes to fetch everything while not hitting Anilist's API rate limiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 332 => Entries 16551-16600\n",
      "16551 entries downloaded\n"
     ]
    }
   ],
   "source": [
    "anime_count = anilist.download_anime_range(os.path.join(data_dir, f\"anime-{today}-raw.csv\"))\n",
    "print(f\"\\n{anime_count} entries downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean/Wrangle Anime Data\n",
    "\n",
    "Probably should have done this during initial fetch...But, let's call it \"low tier data wrangling\" practice :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_raw = f'anime-{today}-raw.csv'\n",
    "csv_anime = f'anime-{today}-clean.csv'\n",
    "\n",
    "anime_header = [\n",
    "    'id', 'title_english', 'title_romaji', 'title_native', 'type', 'format', 'status', \n",
    "    'description', 'start_date', 'end_date', 'season', 'season_year', 'episodes', \n",
    "    'duration_mins', 'country_of_origin', 'genres', 'average_score', 'mean_score', \n",
    "    'popularity', 'source', 'tags', 'studios'\n",
    "]\n",
    "\n",
    "def clean_row(row: list) -> list:\n",
    "    if not row[9] and row[5] == 'MOVIE':\n",
    "        row[9] = row[8] # end_date = start_date\n",
    "    \n",
    "    # add missing seasons\n",
    "    if not row[10]:\n",
    "        m = int(row[8][5:7])\n",
    "        if m >= 9 and m <= 11:\n",
    "            row[10] = 'FALL'\n",
    "        elif m >= 6 and m <= 8:\n",
    "            row[10] = 'SUMMER'\n",
    "        elif m >= 3 and m <= 5:\n",
    "            row[10] = 'SPRING'\n",
    "        else:\n",
    "            row[10] = 'WINTER'\n",
    "    \n",
    "    if not row[11]:\n",
    "        row[11] = row[8][0:4] # year of start date\n",
    "    \n",
    "    row.pop(12) # seasonInt unreliable, removed\n",
    "\n",
    "    # set episodes to nextAiringEpisode and remove the column\n",
    "    if not row[13]:\n",
    "        row[13] = None if row[20] == 'null' else json.loads(row[20])['episode']\n",
    "    \n",
    "    row.pop(20) # nextAiringEpisode not needed anymore, removed\n",
    "\n",
    "    row[20] = [tag['name'] for tag in json.loads(row[21])]\n",
    "    if len(row[21]) > 0:\n",
    "        row[21] = [studio['name'] for studio in json.loads(row[21])]\n",
    "    return row\n",
    "\n",
    "def include_row(row: list) -> bool:\n",
    "    return (row[5] in ['TV', 'MOVIE']\n",
    "        and row[8])\n",
    "\n",
    "# clean each row of raw CSV and expand some entities to separate tables\n",
    "with open(os.path.join(data_dir, csv_anime), 'w+', encoding='utf-8') as of:\n",
    "    anime_writer = csv.writer(of)\n",
    "\n",
    "    with open(os.path.join(data_dir, csv_raw), 'r', encoding='utf-8') as rf:\n",
    "        anime_reader = csv.reader(rf)\n",
    "        next(anime_reader) # skip header\n",
    "        anime_writer.writerow(anime_header)\n",
    "        for row in anime_reader:\n",
    "            if include_row(row):\n",
    "                anime_writer.writerow(clean_row(row))                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
